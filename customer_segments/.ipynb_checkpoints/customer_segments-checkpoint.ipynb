{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "",
  "signature": "sha256:5d52a9f977a14dc0bf7062dd3cd79a07881c6e1a761934c96de5d42e436d306d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Creating Customer Segments"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this project you, will analyze a dataset containing annual spending amounts for internal structure, to understand the variation in the different types of customers that a wholesale distributor interacts with.\n",
      "\n",
      "Instructions:\n",
      "\n",
      "- Run each code block below by pressing **Shift+Enter**, making sure to implement any steps marked with a TODO.\n",
      "- Answer each question in the space provided by editing the blocks labeled \"Answer:\".\n",
      "- When you are done, submit the completed notebook (.ipynb) with all code blocks executed, as well as a .pdf version (File > Download as)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import libraries: NumPy, pandas, matplotlib\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Tell iPython to include plots inline in the notebook\n",
      "%matplotlib inline\n",
      "\n",
      "# Read dataset\n",
      "data = (pd.read_csv(\"wholesale-customers.csv\")).astype(float) #cast to float to remove ICA warning\n",
      "print \"Dataset has {} rows, {} columns\".format(*data.shape)\n",
      "print data.head()  # print the first 5 rows"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dataset has 440 rows, 6 columns\n",
        "   Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicatessen\n",
        "0  12669  9656     7561     214              2674          1338\n",
        "1   7057  9810     9568    1762              3293          1776\n",
        "2   6353  8808     7684    2405              3516          7844\n",
        "3  13265  1196     4221    6404               507          1788\n",
        "4  22615  5410     7198    3915              1777          5185\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Feature Transformation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1)** In this section you will be using PCA and ICA to start to understand the structure of the data. Before doing any computations, what do you think will show up in your computations? List one or two ideas for what might show up as the first PCA dimensions, or what type of vectors will show up as ICA dimensions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Answer: I think that PCA will find that some of the categories like Fresh and Milk are similar and will find a way to group them together in one of its principle components; this might be a surrogate for freshness.  ICA is harder to predict because it is looking to create statistically independent features from the current features.  Maybe ICA might find features that are more like the cuisine/type of food?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "PCA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Apply PCA with the same number of dimensions as variables in the dataset\n",
      "\n",
      "#Normalize data features with centers at 0 [-1,1]\n",
      "#data_normalized = (data - data.mean()) / (data.max() - data.min())\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "n_features = data.shape[1]\n",
      "pca = PCA(n_components = n_features, whiten = True).fit(data)\n",
      "\n",
      "# Print the components and the amount of variance in the data contained in each dimension\n",
      "print pca.components_\n",
      "print pca.explained_variance_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-0.97653685 -0.12118407 -0.06154039 -0.15236462  0.00705417 -0.06810471]\n",
        " [-0.11061386  0.51580216  0.76460638 -0.01872345  0.36535076  0.05707921]\n",
        " [-0.17855726  0.50988675 -0.27578088  0.71420037 -0.20440987  0.28321747]\n",
        " [-0.04187648 -0.64564047  0.37546049  0.64629232  0.14938013 -0.02039579]\n",
        " [ 0.015986    0.20323566 -0.1602915   0.22018612  0.20793016 -0.91707659]\n",
        " [-0.01576316  0.03349187  0.41093894 -0.01328898 -0.87128428 -0.26541687]]\n",
        "[ 0.45961362  0.40517227  0.07003008  0.04402344  0.01502212  0.00613848]\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2)** How quickly does the variance drop off by dimension? If you were to use PCA on this dataset, how many dimensions would you choose for your analysis? Why?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "Answer: The output above shows how we can create the new PCA feature/dimensions.  If we multiply the rows by the original data (as columns) we'll get our tranformed principle components.  The pca.explained_variance_ratio (which is the vector of the eigenvalues for this analysis) shows that the first 2 components explain a lot of the variance (0.4596 and 0.40517).  After that the components don't seem to describe much of the variance at all (<= 0.07004).  So, I would probably only use the first 2 components in my analysis."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3)** What do the dimensions seem to represent? How can you use this information?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "Answer: Each row in the pca.components output represents one of the principle components and each column represents one of the original features input into PCA.  This table seems to suggest that Fresh is one strong (negative) component (-0.9765), and the 2nd component seems to be a pretty strong mix of Milk, Grocery, and a little Detergent-Paper (0.515, 0.764, and 0.365 respectively).  I'll call the first component Fresh Food and the 2nd component Regular Groceries."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "ICA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Fit an ICA model to the data\n",
      "# Note: Adjust the data to have center at the origin first!\n",
      "\n",
      "#Normalize data features with centers at 0 [-1,1]\n",
      "data_normalized = (data - data.mean()) / (data.max() - data.min())\n",
      "\n",
      "#Run ICA on normalized features and print resulting components\n",
      "from sklearn.decomposition import FastICA\n",
      "ica = FastICA(n_components = n_features, whiten = True, \n",
      "              random_state = 1).fit(data_normalized)\n",
      "\n",
      "# Print the independent components\n",
      "print ica.components_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.03349093 -0.17010275 -1.11680419  0.08908118  1.15131334  0.27470505]\n",
        " [-0.04334463 -0.01612397 -0.05567124 -0.03176554  0.0208026   0.86734557]\n",
        " [ 0.02354237 -0.13809602  0.59968365  0.02472737 -0.03645177 -0.07055503]\n",
        " [ 0.09702365  0.0103289  -0.07180229 -0.67818102  0.02261901  0.28534266]\n",
        " [-0.01725562 -0.72298675  0.53934345  0.02210125 -0.13576917  0.2903145 ]\n",
        " [ 0.44590403 -0.06317793 -0.05851318 -0.04118495  0.0847063  -0.04985377]]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**4)** For each vector in the ICA decomposition, write a sentence or two explaining what sort of object or property it corresponds to. What could these components be used for?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "Answer: \n",
      "\n",
      "1) This vector takes a lot of negative effect from Grocery (-1.116) and a lot of positive effect from Detergents_Paper (1.151). Maybe this vector would be good for describing how much non-food items a store buys.\n",
      "\n",
      "2) This vector takes a lot of its effect from Delicatessen (0.867).  This one just seems to be the Delicatessen effect with a little bit from others to make it statistically independent.\n",
      "\n",
      "3) This vector is positively influenced by Grocery (0.599) and negatively a little by Milk (-0.138).  This one might represent how much the store buys of non-dairy Grocery items.\n",
      "\n",
      "4) This vector is negatively influenced by Frozen (-0.678) and slightly positively influenced by Delicatessen (0.285).  The negative signal from this vector could describe how much the store buys frozen foods, compared to refrigerated/ready-made foods in a Deli.\n",
      "\n",
      "5) This vector is negatively influenced by Milk (-0.723) and positively by Grocery (0.539).  This negative signal of this vector could describe how much Milk is bought compared to groceries.  That might be an important ratio...\n",
      "\n",
      "6) This vector is positively influenced by Fresh (0.446), with traces of the others...\n",
      "\n",
      "These components could be used to cluster the stores by the type of items they buy, perhaps regardless of the quantity.  They could also be used to describe different (statistically independent) types of store purchasing behaviours."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Clustering\n",
      "\n",
      "In this section you will choose either K Means clustering or Gaussian Mixed Models clustering, which implements expectation-maximization. Then you will sample elements from the clusters to understand their significance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Choose a Cluster Type\n",
      "\n",
      "**5)** What are the advantages of using K Means clustering or Gaussian Mixture Models?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Answer: Gaussian Mixture Models find clusters centers much like K-means, but Gaussian Mixture Models have the advantage of being able to change the size/shape of their probability density functions to better match the cluster shape.  For example, if we had a cluster that was shaped like a horizontal oval, Gaussian Mixture models would be able to change the variance of the gaussian distribution to more accurately include the long/wide shape of that cluster.  A K-means cluster just looks for the distance between the cluster center and the closests points.  The k-means algorithm would be faster, but might not cluster the data as well.  The other big advantage of the K-means algorithm is that the cluster boundaries will often be linear and therefore potentially easier for a human to understand.  Since I'm doing this analysis for a business client, I'll try the simpler K-means first to see if the result looks reasonable, and then try GMM as a comparison (you can see commented code below for the GMM solution as well)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**6)** Below is some starter code to help you visualize some cluster data. The visualization is based on [this demo](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html) from the sklearn documentation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Import clustering modules\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn.mixture import GMM"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: First we reduce the data to two dimensions using PCA to capture variation\n",
      "reduced_data = PCA(n_components = 2, whiten = True).fit_transform(data)\n",
      "print reduced_data[:10]  # print upto 10 elements"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-0.05066239  0.13161505]\n",
        " [ 0.34502287  0.33556674]\n",
        " [ 0.37738285  0.21406486]\n",
        " [-0.07718708 -0.5212911 ]\n",
        " [-0.83067886 -0.17928035]\n",
        " [ 0.2155776  -0.07967954]\n",
        " [ 0.05576966 -0.16710073]\n",
        " [ 0.34874672  0.11866355]\n",
        " [ 0.52313722 -0.18311407]\n",
        " [ 0.37595155  1.11903068]]\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Implement your clustering algorithm here, and fit it to the reduced data for visualization\n",
      "# The visualizer below assumes your clustering object is named 'clusters'\n",
      "\n",
      "np.random.seed(1)\n",
      "#generate a k-means score to determine where the elbow is in the curve\n",
      "print \"Looking for Elbow in K-Means score\"\n",
      "for k in range(2, 8):   #do this for 2 <= k <=7\n",
      "    clusters = KMeans(n_clusters = k).fit(reduced_data)\n",
      "    clusters_score = round(clusters.score(reduced_data),2) \n",
      "    print \"k=%d; Score: %r \" % (k, clusters_score)\n",
      "\n",
      "#Looks like there's an elbow around 3 clusters\n",
      "#clusters = GMM(n_components = 3).fit(reduced_data)\n",
      "clusters = KMeans(n_clusters = 3).fit(reduced_data)\n",
      "    \n",
      "print clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Looking for Elbow in K-Means score\n",
        "k=2; Score: -610.4 \n",
        "k=3; Score: -380.91 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "k=4; Score: -277.41 \n",
        "k=5; Score: -207.38 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "k=6; Score: -169.16 \n",
        "k=7; Score: -136.67 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10,\n",
        "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
        "    verbose=0)\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the decision boundary by building a mesh grid to populate a graph.\n",
      "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
      "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
      "hx = (x_max-x_min)/1000.\n",
      "hy = (y_max-y_min)/1000.\n",
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
      "\n",
      "# Obtain labels for each point in mesh. Use last trained model.\n",
      "Z = clusters.predict(np.c_[xx.ravel(), yy.ravel()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Find the centroids for KMeans or the cluster means for GMM \n",
      "\n",
      "#centroids = clusters.means_    #GMM\n",
      "centroids = clusters.cluster_centers_   #Kmeans\n",
      "print centroids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.32398252 -0.25421161]\n",
        " [-1.86890029 -0.36902956]\n",
        " [ 0.10439573  2.12063212]]\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Put the result into a color plot\n",
      "Z = Z.reshape(xx.shape)\n",
      "plt.figure(1)\n",
      "plt.clf()\n",
      "plt.imshow(Z, interpolation='nearest',\n",
      "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
      "           cmap=plt.cm.Paired,\n",
      "           aspect='auto', origin='lower')\n",
      "\n",
      "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
      "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
      "            marker='x', s=169, linewidths=3,\n",
      "            color='w', zorder=10)\n",
      "plt.title('Clustering on the wholesale grocery dataset (PCA-reduced data)\\n'\n",
      "          'Centroids are marked with white cross')\n",
      "plt.xlim(x_min, x_max)\n",
      "plt.ylim(y_min, y_max)\n",
      "plt.xticks(())\n",
      "plt.yticks(())\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAELCAYAAAAY3LtyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIihSlERESkMIWIiIgUphAREZHC/h+1cxnwCHTp+gAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10858f50>"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**7)** What are the central objects in each cluster? Describe them as customers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "Answer: \n",
      "There are 3 cluster centers:  \n",
      "The first center is at (0.3239, -0.2542) which is the white X in the blue region of the plot.  Since in the first principle component represents Fresh Food and the 2nd represents groceries, this would be a cluster of stores that buy more fresh food and not as many groceries.  Perhaps these are fruit stands, or health food stores...\n",
      "\n",
      "The 2nd cluster center is at (-1.8689, -0.36) which is the white X in the orange region of the plot.  These stores are don't really buy fresh foods, and don't buy as many groceries either.  Maybe these are convenience stores?\n",
      "\n",
      "The 3rd cluster center is at (0.1044, 2.121) which is the white X in the brown region of the plot.  These stores buy fresh foods, but they are mostly huge purchasers of groceries.  I would describe these stores as being larger grocery stores like Wal-mart."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Conclusions\n",
      "\n",
      "** 8)** Which of these techniques did you feel gave you the most insight into the data?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "Answer: I thought that I got the most insight into the data with PCA and K-means.  PCA did a great job of quickly telling me what the most important (combined) factors were in the feature set.  K-means output was much more human understandable (3 separate regions).  Those 3 regions helped me think about the customers and what was different about them.  I could intuitively tell what types of businesses they were based on the clustering.  ICA and GMM were new concepts to me and I was hoping that they would give new and interesting insights, but the information/output they gave seemed hard to interpret.  ICA didn't seem to add much value to the interpretation of the data, and didn't reduce the dimensionality of the problem either.  If I had just taken 2 of the features like I did with PCA, I would have lost a lot of the information in the feature set.  GMM's output plot, while you can't see it in this final version, was weird to look at.  The clusters were roundish and one of the clusters was divided by another cluster running right through the middle of it.  Luckily, PCA allowed me to get the feature space down to 2 dimensions so I could see this with my eye and decide which method I liked better for this problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "**9)** How would you use that technique to help the company design new experiments?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Answer: Using the cluster assignment in the experiment would allow the company to determine if there were different responses from different customer segments, instead of averaging results over the entire customer population.  They might run an A/B test in just one customer segment at a time in the future to get a clearer understanding of each customer segments' needs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**10)** How would you use that data to help you predict future customer needs?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Answer: If we ran small A/B experiments as suggested above in targetted segments, then the business would be able to extend the improved results to the rest of the segment, without fear of hurting results in the other segments (like what happened to them in the past).  Perhaps they might try to optimize profit for the biggest segment, or they might try to grow their business in a smaller customer segment through this type of experimentation.  Over time, this might allow the company to find that these different segments prefer different types of service/products thus helping them to better serve their customers, improve customer loyalty, and increase the companies profits.\n",
      "\n",
      "If by future customer needs, the question is referring to new customers, then the business would be able to assign the new customer to a cluster pretty quickly and then be able to serve that customer like other similar customers.  This would help to retain those new customers."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}