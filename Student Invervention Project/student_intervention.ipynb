{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "",
  "signature": "sha256:c7cd338f7819394c7c71f3cd8f47a4b37904b958c42dd8584c06bb32a05a7743"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Project 2: Supervised Learning\n",
      "### Building a Student Intervention System"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. Classification vs Regression\n",
      "\n",
      "Your goal is to identify students who might need early intervention - which type of supervised machine learning problem is this, classification or regression? Why?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. Exploring the Data\n",
      "\n",
      "Let's go ahead and read in the student dataset first.\n",
      "\n",
      "_To execute a code cell, click inside it and press **Shift+Enter**._"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import libraries\n",
      "import numpy as np\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read student data\n",
      "student_data = pd.read_csv(\"student-data.csv\")\n",
      "print \"Student data read successfully!\"\n",
      "# Note: The last column 'passed' is the target/label, all other are feature columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Student data read successfully!\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, can you find out the following facts about the dataset?\n",
      "- Total number of students\n",
      "- Number of students who passed\n",
      "- Number of students who failed\n",
      "- Graduation rate of the class (%)\n",
      "- Number of features\n",
      "\n",
      "_Use the code block below to compute these values. Instructions/steps are marked using **TODO**s._\n",
      "\n",
      "**Answer:**  See iPython notebook code/output for answers to this question."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Compute desired values - replace each '?' with an appropriate expression/function call\n",
      "n_students = student_data.shape[0]\n",
      "n_features = student_data.shape[1] - 1\n",
      "n_passed = np.sum(student_data['passed'] == 'yes')\n",
      "n_failed = np.sum(student_data['passed'] == 'no')\n",
      "grad_rate = float(n_passed) / n_students\n",
      "print \"Total number of students: {}\".format(n_students)\n",
      "print \"Number of students who passed: {}\".format(n_passed)\n",
      "print \"Number of students who failed: {}\".format(n_failed)\n",
      "print \"Number of features: {}\".format(n_features)\n",
      "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of students: 395\n",
        "Number of students who passed: 265\n",
        "Number of students who failed: 130\n",
        "Number of features: 30\n",
        "Graduation rate of the class: 0.67%\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. Preparing the Data\n",
      "In this section, we will prepare the data for modeling, training and testing.\n",
      "\n",
      "### Identify feature and target columns\n",
      "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
      "\n",
      "Let's first separate our data into feature and target columns, and see if any features are non-numeric.<br/>\n",
      "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict.\n",
      "\n",
      "**Answer:**  See iPython notebook code/output for answers to this question."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract feature (X) and target (y) columns\n",
      "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
      "target_col = student_data.columns[-1]  # last column is the target/label\n",
      "print \"Feature column(s):-\\n{}\".format(feature_cols)\n",
      "print \"Target column: {}\".format(target_col)\n",
      "\n",
      "X_all = student_data[feature_cols]  # feature values for all students\n",
      "y_all = student_data[target_col]  # corresponding targets/labels\n",
      "print \"\\nFeature values:-\"\n",
      "print X_all.head()  # print the first 5 rows"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Feature column(s):-\n",
        "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
        "Target column: passed\n",
        "\n",
        "Feature values:-\n",
        "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
        "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
        "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
        "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
        "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
        "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
        "\n",
        "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
        "0   ...       yes       no        no       4         3     4    1    1      3   \n",
        "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
        "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
        "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
        "4   ...       yes       no        no       4         3     2    1    2      5   \n",
        "\n",
        "  absences  \n",
        "0        6  \n",
        "1        4  \n",
        "2       10  \n",
        "3        2  \n",
        "4        4  \n",
        "\n",
        "[5 rows x 30 columns]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Preprocess feature columns\n",
      "\n",
      "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
      "\n",
      "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
      "\n",
      "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Preprocess feature columns\n",
      "def preprocess_features(X):\n",
      "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
      "\n",
      "    # Check each column\n",
      "    for col, col_data in X.iteritems():\n",
      "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
      "        if col_data.dtype == object:\n",
      "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
      "        # Note: This should change the data type for yes/no columns to int\n",
      "\n",
      "        # If still non-numeric, convert to one or more dummy variables\n",
      "        if col_data.dtype == object:\n",
      "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'school' => 'school_GP', 'school_MS'\n",
      "\n",
      "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
      "\n",
      "    return outX\n",
      "\n",
      "X_all = preprocess_features(X_all)\n",
      "print \"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed feature columns (48):-\n",
        "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Split data into training and test sets\n",
      "\n",
      "So far, we have converted all _categorical_ features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First, decide how many training vs test samples you want\n",
      "num_all = student_data.shape[0]  # same as len(student_data)\n",
      "num_train = 300  # This gets varied to build tables (all <=300)\n",
      "num_test = num_all - num_train    #need to keep this a constant for future work\n",
      "\n",
      "\n",
      "# TODO: Then, select features (X) and corresponding labels (y) for the training and test sets\n",
      "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
      "\n",
      "#create a random (uniform dist.) series to be used for sorting\n",
      "np.random.seed(seed=1)\n",
      "rand_series = np.random.random_sample(student_data.shape[0])\n",
      "X_all.loc[:,'sort_order'] = pd.Series(rand_series, index = X_all.index)\n",
      "y_all = pd.DataFrame(y_all, index = y_all.index)  #turn y_all to dataframe instead of series\n",
      "y_all.loc[:,'sort_order'] = pd.Series(rand_series, index = y_all.index)\n",
      "\n",
      "#sort the dataframe by the new random number column\n",
      "X_rand = X_all.sort(['sort_order'], ascending=1)\n",
      "y_rand = y_all.sort(['sort_order'], ascending=1)\n",
      "\n",
      "X_train = X_rand.iloc[0:num_train,:-1]\n",
      "y_train = y_rand.iloc[0:num_train,0]\n",
      "X_test = X_rand.iloc[num_train:,:-1]\n",
      "y_test = y_rand.iloc[num_train:,0]\n",
      "print \"Training set: {} samples\".format(X_train.shape[0])\n",
      "print \"Test set: {} samples\".format(X_test.shape[0])\n",
      "# Note: If you need a validation set, extract it from within training data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set: 300 samples\n",
        "Test set: 95 samples\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4. Training and Evaluating Models\n",
      "Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model:\n",
      "\n",
      "- What are the general applications of this model? What are its strengths and weaknesses?\n",
      "- Given what you know about the data so far, why did you choose this model to apply?\n",
      "- Fit this model to the training data, try to predict labels (for both training and test sets), and measure the F<sub>1</sub> score. Repeat this process with different training set sizes (100, 200, 300), keeping test set constant.\n",
      "\n",
      "Produce a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.\n",
      "\n",
      "Note: You need to produce 3 such tables - one for each model.\n",
      "\n",
      "**Answer:**  (see PDF file for tables with performance results)\n",
      "\n",
      "DecisionTreeClassifier:\n",
      "This model can be used as a classification algorithm.  It generates a decision tree that classifies examples based on Boolean nodes.  The benefit of a model like this, if it fit the data well, would be that the fitted model would be able to be printed off and is human readable.  This human readability would allow the school to fit the model once, then avoid using the costly computer resources in the future to determine if students might need additional help and guidance.  There are a couple of weaknesses of decision trees though.  They are extremely sensitive to overfitting the data.  While that can be managed through parameter tuning, it is hard to have high confidence that the final model will generalize optimally.  Also, since each leaf is dependent upon all of the decision nodes above it in the tree, it assumes variable interactions.  Variables in a data set are not necessarily interdependent.  Decision trees also have problems predicting out-of-sample instances.  If a feature set/outcome pair is not in the training data set, it is very hard for a decision tree to generalize well when prediction that type of outcome.\n",
      "\n",
      "I chose this model because it was simple, and potentially human readable.  If it fit the data well, that would be a good model to use according to Occam\u2019s razor.\n",
      "\n",
      "SVC:\n",
      "SVC stands for support vector classifier and it is used to classify data use a support vector algorithm framework.  Support vector machines are a good option to explore for a couple of reasons: (1) they can fit very complex data if the correct kernel is selected; (2) they are relatively easy to tune (using grid search to change values for C and gamma) to minimize overfitting; (3) SVM tends to have relatively good out-of-sample generalization ; (4) SVM will give a unique solution, because the However, SVM has some drawbacks.  Especially compared to a decision tree, SVM is not human readable, or interpretable.  The school would have to just \u201ctrust the model\u201d which might seem like a black box to them.  The other difficulty with SVM is selecting the kernel.  It is difficult beforehand to know what kernel will create the best data fit.  In this case, I\u2019m starting with just the default \u2018rbf\u2019 kernel to see if the model will work fairly well.  \n",
      "\n",
      "After looking at the results from the decision tree model, it looked like it trained pretty quickly, but didn\u2019t seem very accurate.  So, I thought that SVM might be able to fit the data better based on its advantages described above.  This would be a sacrifice in model interpretation, but at the end of the day the school really wants to know which students to work with.\n",
      "\n",
      "AdaBoostClassifier:\n",
      "Boosting is an algorithm that uses the weighted average of many weak classifiers (in the case of AdaBoost, its linear hyperplanes) to predict the output class.  One advantage of this algorithm is that its weighting is updated during each iteration to incentivize the correctly classify points in the next iteration that it got wrong in the previous iteration.  In theory this helps the algorithm find a good fit to the data.  Also, since it is an ensemble method (averaging outputs of other models) it has the property that is should generalize well.  The real drawback for this approach is the time it takes to train.  Essentially, a linear hyperplane decision boundary gets fit to the data during each iteration, and then those hyperplanes are weighted averaged together.  That\u2019s a lot of computation that the school might not want due to their concerns over cost and processing time.\n",
      "\n",
      "While watching the Udacity training, Boosting was an algorithm I had heard of before but didn\u2019t understand yet. I was intrigued by the claim that it is resistant to overfitting, and thought that I would try it to see if it would have a better generalization error on the test set.  I guessed that this model would have the best F1 score, even if it took a long time to train.  That didn\u2019t end up being the case, at least not with the default parameters being used.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Train a model\n",
      "import time\n",
      "\n",
      "def train_classifier(clf, X_train, y_train):\n",
      "    print \"Training {}...\".format(clf.__class__.__name__)\n",
      "    start = time.time()\n",
      "    clf.fit(X_train, y_train)\n",
      "    end = time.time()\n",
      "    print \"Done!\\nTraining time (secs): {:.3f}\".format(end - start)\n",
      "\n",
      "# TODO: Choose a model, import it and instantiate an object\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "clf = DecisionTreeClassifier()\n",
      "\n",
      "# Fit model to training data\n",
      "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n",
      "#print clf  # you can inspect the learned model by printing it"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training DecisionTreeClassifier...\n",
        "Done!\n",
        "Training time (secs): 0.003\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Predict on training set and compute F1 score\n",
      "from sklearn.metrics import f1_score\n",
      "\n",
      "def predict_labels(clf, features, target):\n",
      "    print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
      "    start = time.time()\n",
      "    y_pred = clf.predict(features)\n",
      "    end = time.time()\n",
      "    print \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start)\n",
      "    return f1_score(target.values, y_pred, pos_label='yes')\n",
      "\n",
      "train_f1_score = float(predict_labels(clf, X_train, y_train))\n",
      "print \"F1 score for training set: {}\".format(train_f1_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Predicting labels using DecisionTreeClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.001\n",
        "F1 score for training set: 1.0\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Predict on test data\n",
      "print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Predicting labels using DecisionTreeClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.000\n",
        "F1 score for test set: 0.755555555556\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Train and predict using different training set sizes\n",
      "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
      "    print \"------------------------------------------\"\n",
      "    print \"Training set size: {}\".format(len(X_train))\n",
      "    train_classifier(clf, X_train, y_train)\n",
      "    print \"F1 score for training set: {}\".format(predict_labels(clf, X_train, y_train))\n",
      "    print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))\n",
      "\n",
      "# TODO: Run the helper function above for desired subsets of training data\n",
      "# Note: Keep the test set constant\n",
      "\n",
      "def get_clf_metrics(clf, X_train, y_train, X_test, y_test):\n",
      "    # get random sample of X_train\n",
      "    np.random.seed(seed=1)\n",
      "    train_sizes = [300, 200, 100]\n",
      "    #loop through the training set sizes for which we want metrics\n",
      "    for t_size in train_sizes:\n",
      "        #pick random subset of the X_train and y_train data and pass to train_predict    \n",
      "        rand_sample = np.random.choice(a = X_train.index, size = t_size, \n",
      "                                       replace = False)\n",
      "        train_predict(clf, X_train[X_train.index.isin(rand_sample)], \n",
      "                               y_train[y_train.index.isin(rand_sample)], \n",
      "                                       X_test, y_test)\n",
      "\n",
      "get_clf_metrics(clf, X_train, y_train, X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "------------------------------------------\n",
        "Training set size: 300\n",
        "Training DecisionTreeClassifier...\n",
        "Done!\n",
        "Training time (secs): 0.005\n",
        "Predicting labels using DecisionTreeClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.001\n",
        "F1 score for training set: 1.0\n",
        "Predicting labels using DecisionTreeClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.000\n",
        "F1 score for test set: 0.740740740741\n",
        "------------------------------------------\n",
        "Training set size: 200\n",
        "Training DecisionTreeClassifier...\n",
        "Done!\n",
        "Training time (secs): 0.003\n",
        "Predicting labels using DecisionTreeClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.001\n",
        "F1 score for training set: 1.0\n",
        "Predicting labels using DecisionTreeClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.000\n",
        "F1 score for test set: 0.704\n",
        "------------------------------------------\n",
        "Training set size: 100\n",
        "Training DecisionTreeClassifier...\n",
        "Done!\n",
        "Training time (secs): 0.002\n",
        "Predicting labels using DecisionTreeClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.000\n",
        "F1 score for training set: 1.0\n",
        "Predicting labels using DecisionTreeClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.001\n",
        "F1 score for test set: 0.794326241135\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Train and predict using two other models\n",
      "\n",
      "#test a support vector machine classifier\n",
      "from sklearn.svm import SVC\n",
      "SVM_clf = SVC()\n",
      "get_clf_metrics(SVM_clf, X_train, y_train, X_test, y_test)\n",
      "\n",
      "#test a boosting classifier\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "boost_clf = AdaBoostClassifier(n_estimators=100)\n",
      "get_clf_metrics(boost_clf, X_train, y_train, X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "------------------------------------------\n",
        "Training set size: 300\n",
        "Training SVC...\n",
        "Done!\n",
        "Training time (secs): 0.011\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.008\n",
        "F1 score for training set: 0.859649122807\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.003\n",
        "F1 score for test set: 0.818181818182\n",
        "------------------------------------------\n",
        "Training set size: 200\n",
        "Training SVC...\n",
        "Done!\n",
        "Training time (secs): 0.003\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.002\n",
        "F1 score for training set: 0.867313915858\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.001\n",
        "F1 score for test set: 0.820512820513\n",
        "------------------------------------------\n",
        "Training set size: 100\n",
        "Training SVC...\n",
        "Done!\n",
        "Training time (secs): 0.001\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.001\n",
        "F1 score for training set: 0.873417721519\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.001\n",
        "F1 score for test set: 0.820512820513\n",
        "------------------------------------------"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training set size: 300\n",
        "Training AdaBoostClassifier...\n",
        "Done!\n",
        "Training time (secs): 0.085"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting labels using AdaBoostClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.010\n",
        "F1 score for training set: 0.861244019139\n",
        "Predicting labels using AdaBoostClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.007\n",
        "F1 score for test set: 0.777777777778\n",
        "------------------------------------------\n",
        "Training set size: 200\n",
        "Training AdaBoostClassifier...\n",
        "Done!\n",
        "Training time (secs): 0.072"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting labels using AdaBoostClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.009\n",
        "F1 score for training set: 0.879377431907\n",
        "Predicting labels using AdaBoostClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.007\n",
        "F1 score for test set: 0.69696969697\n",
        "------------------------------------------\n",
        "Training set size: 100\n",
        "Training AdaBoostClassifier...\n",
        "Done!\n",
        "Training time (secs): 0.079"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Predicting labels using AdaBoostClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.011\n",
        "F1 score for training set: 1.0\n",
        "Predicting labels using AdaBoostClassifier...\n",
        "Done!\n",
        "Prediction time (secs): 0.011\n",
        "F1 score for test set: 0.816901408451\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 5. Choosing the Best Model\n",
      "\n",
      "- Based on the experiments you performed earlier, in 1-2 paragraphs explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?\n",
      "- In 1-2 paragraphs explain to the board of supervisors in layman's terms how the final model chosen is supposed to work (for example if you chose a Decision Tree or Support Vector Machine, how does it make a prediction).\n",
      "- Fine-tune the model. Use Gridsearch with at least one important parameter tuned and with at least 3 settings. Use the entire training set for this.\n",
      "- What is the model's final F<sub>1</sub> score?\n",
      "\n",
      "**Answer:**  \n",
      "\n",
      "I tried decision trees, support vector machines (SVM) and the adaboost algorithm.  Out of the 3 I chose the SVM model as the best model to refine and optimize.  The biggest reason for this choice was the F1 scores obtained from the model compared to the other 2.  The F1 score on the test set was consistently higher than 0.81 regardless of training set size.  Decision trees gave fairly consistent F1 scores in the 0.70\u2019s (0.794, 0.704, 0.741) which seems to suggest that either the model parameters weren\u2019t tuned well enough or the model itself didn\u2019t fit the data as well.  The AdaBoost model seems to have mixed F1 test results (0.817, 0.697, 0.778).  This suggested to me that perhaps the model was getting lucky (or unlucky) based on the subset data points it was selecting from the 300 training points.  So, I didn\u2019t expect this model to generalize well in the future with new data points.\n",
      "\n",
      "On top of F1 performance, the processing time and data storage requirements needed to be considered for this project.  As mentioned in the discussion of the AdaBoost algorithm above, it takes a lot of time to train and predict so it was noticeably worse that the other 2 options regardless of training set size.  The SVM model and the Decision Tree model had quick training times (less than 0.009 for all options tried), and had even faster prediction times (less than 0.001 seconds).  The prediction time is more important over the long run because this is what the school will be using regularly to make predictions.\n",
      "Since the end user wants to minimize the data storage and processing time while maximizing the F1 score, I thought the 100 or 200 point SVM model would likely work well for them.\n",
      "\n",
      "\n",
      "**Answer:**  (see PDF answers for charts referred to in the text below)\n",
      "\n",
      "For simple problems support vector machines look for a line that will best split the data into categories while keeping as much space a possible between data points in different categories.  If you had a graph where all the points on the left were red and all the points on the right were green, it would create a (semi-)vertical line right in between the farthest right red point(s) and the farthest left green point(s).  I say \u201cright between\u201d because SVM tries to maximize margin, which is another way of saying that it creates a dividing line with the most space between the line and the closest points of each category (see chart below for how this looks in a 2D graph).\n",
      "\n",
      "The way SVM finds this solution is by maximizing the distance between the blue decision boundary and the closest data points of each category.  You can see below that if we had selected a different blue decision boundary, that the data would still be separated, but would NOT maximize this distance from the data points near the boundary.\n",
      "\n",
      "Once we have this blue decision boundary (essentially an equation for the line that splits the data) we can classify new data points by determining which side of the line that data point lies on.  For example if we were give the data point (4.5, 3) we would know it was on the right side of the vertical line and classify it as green.\n",
      "\n",
      "When data gets more complex solving this problem gets more difficult.  Let\u2019s use the chart below as an example\n",
      "\n",
      "It is impossible to draw a straight line that will separate the \u201cgood\u201d and \u201cbad\u201d points in this chart.  So support vector machines uses what is called the \u201ckernel trick\u201d.   This kernel essentially adds another layer to the data.  Think about this data being in 3 dimensions instead of 2 where the green points are closer to the reader (coming out of the page) and the red points are farther away (behind the page).  If that\u2019s true, then we can split the data using the plane of the page.  It gets hard to visualize this if we have more than to variables we\u2019re using to predict the output but the effect is still the same.  If a final answer were put on our example chart, it would look similar to this hand-drawn curve below.  We would predict new examples similar to the linear example.  A point at (4,6) is above and to the right of the blue decision boundary so it would be classified as \u201cBad\u201d.\n",
      "\n",
      "**Answer:**  (see PDF answers for final results chart referred to in the text below)\n",
      "I fine-tuned the model using grid search for the kernel, C and gamma parameters.  Here\u2019s the parameter space I explored to get my final model parameters and scores:\n",
      "\n",
      "\u2022\tKernel = ('linear', 'rbf', 'sigmoid')\n",
      "\n",
      "\u2022\tC = (.001, .01, 0.1, 1.0, 10.0, 1000.0)\n",
      "\n",
      "\u2022\tGamma = (.001, .01, 0.1, 1.0, 10.0, 1000.0)\n",
      "\n",
      "When the grid search was complete, its cross-validated F1 score was 0.8013766... and the best fit parameters were kernel = \u2018rbf\u2019, C = 10.0, and gamma = 0.001.\n",
      " \n",
      "After tuning the parameters, and looking at the performance metrics, it appears that the training set of 200 with the optimized parameters would best fit the school\u2019s needs.  It doesn\u2019t take long (0.006 combined seconds), its F1 score is the highest on the test set, and it doesn\u2019t require the whole data set for information storage.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# TODO: Fine-tune your model and report the best F1 score\n",
      "\n",
      "# Set up the parameters we wish to tune\n",
      "# use logarithmic grid from 10^(-3) to 10^3 for C and gamma\n",
      "parameters = {'C':(.001, .01, 0.1, 1.0, 10.0, 1000.0), \n",
      "              'kernel': ('linear', 'rbf', 'sigmoid'), \n",
      "              'gamma': (.001, .01, 0.1, 1.0, 10.0, 1000.0)}\n",
      "    \n",
      "\n",
      "# Make an F1 scoring function\n",
      "from sklearn.metrics import make_scorer\n",
      "scoring_function = make_scorer(f1_score, pos_label='yes')\n",
      "\n",
      "# Make the GridSearchCV object\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "grid_result = GridSearchCV(SVM_clf, parameters, scoring_function, cv=5)\n",
      "grid_result.fit(X_train, y_train)\n",
      "\n",
      "print grid_result.best_score_\n",
      "print grid_result.best_params_\n",
      "\n",
      "#put the best GridSearchCV results into the SVM classifier and get metrics\n",
      "SVM_clf = SVC(kernel = 'rbf', C = 10, gamma = .001)\n",
      "get_clf_metrics(SVM_clf, X_train, y_train, X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.80137664139\n",
        "{'kernel': 'rbf', 'C': 10.0, 'gamma': 0.001}\n",
        "------------------------------------------\n",
        "Training set size: 300\n",
        "Training SVC...\n",
        "Done!\n",
        "Training time (secs): 0.007\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.004\n",
        "F1 score for training set: 0.822757111597\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.002\n",
        "F1 score for test set: 0.820512820513\n",
        "------------------------------------------\n",
        "Training set size: 200\n",
        "Training SVC...\n",
        "Done!\n",
        "Training time (secs): 0.003\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.003\n",
        "F1 score for training set: 0.842443729904\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.002\n",
        "F1 score for test set: 0.8375\n",
        "------------------------------------------\n",
        "Training set size: 100\n",
        "Training SVC...\n",
        "Done!\n",
        "Training time (secs): 0.001\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.000\n",
        "F1 score for training set: 0.853503184713\n",
        "Predicting labels using SVC...\n",
        "Done!\n",
        "Prediction time (secs): 0.001\n",
        "F1 score for test set: 0.820512820513\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}